# Calculate the mean and variance of the outcome
(mean_bikes <- mean(bikesJuly$cnt))
(var_bikes <- var(bikesJuly$cnt))
# Fit the model
bike_model <- glm(fmla, data = bikesJuly, family = quasipoisson)
# Fit the model
# Notese que se usa como familia quasipoisson
bike_model <- glm(fmla, data = bikesJuly, family = quasipoisson)
# Call glance
#Usamos glance para obtener la desviacion y la desviacion nula para poder hacer el pseudo R2 y ver si nuestro modelo es bueno
(perf <- glance(bike_model))
# Calculate pseudo-R-squared
#El modelo es mas o menos bueno con 0.7 de pseudo R2
(pseudoR2 <- 1 - (perf$deviance/perf$null.deviance))
# Calculate pseudo-R-squared
#El modelo es mas o menos bueno con 0.8 de pseudo R2
(pseudoR2 <- 1 - (perf$deviance/perf$null.deviance))
#Fit a model of sparrow survival probability
#Se prueba la regresion logistica
#Esta regresion esta entre 0 y 1, utiliza el pseudo R2 para ver si se tiene un buen modelo
sparrow = readRDS("/home/marlon/Documents/machine_learning_R/Supervised Learning in R: Regression/sparrow.rds")
library(broom)
# sparrow is in the workspace
summary(sparrow)
# Create the survived column
sparrow$survived <- sparrow$status == "Survived" #Se reemplaza por true o false
# Create the formula
(fmla <- survived ~ total_length + weight + humerus)
# Fit the logistic regression model
sparrow_model <- glm(fmla, data = sparrow, family = binomial)
# Call summary
summary(sparrow_model)
# Call glance
(perf <- glance(sparrow_model)) #Permite sacar facilamente los pseudo R2
# Calculate pseudo-R-squared
(pseudoR2 <- 1 - (perf$deviance/ perf$null.deviance)) # No es un buen modelo porque no se acerca a 1
#Se hacen predicciones con el modelo de regresion logistica
#Se prueba la curva de ganancia
#Entre mas cerca este de la curva de ganancia es mejor
#GainCurvePlot(frame, xvar, truthVar, title)
# frame: data frame with prediction column and ground truth column
# xvar: the name of the column of predictions (as a string)
# truthVar: the name of the column with actual outcome (as a string)
# title: a title for the plot (as a string)
# sparrow is in the workspace
summary(sparrow)
# sparrow_model is in the workspace
summary(sparrow_model)
# Make predictions
sparrow$pred <- predict(sparrow_model, type = "response")
library(WVPlots)
# Look at gain curve
GainCurvePlot(sparrow, "pred", "survived", "sparrow survival model")
# You see from the gain curve that the model follows the wizard curve for about the first 30% of the data, identifying about 45% of the surviving sparrows with only a few false positives.
#Poisson and Quasipoisson
#Se usa para predecir datos de conteo
#Fit a model to predict bike rental counts
library(miceadds)
load.Rdata("/home/marlon/Documents/machine_learning_R/Supervised Learning in R: Regression/Bikes.RData", "bikesJuly")
#Para que hacemos estas variables???????
outcome = "cnt"
vars = c("hr","holiday", "workingday", "weathersit", "temp", "atemp", "hum", "windspeed")
#Las usamos para simplificar la formula que se va a utilizar
# bikesJuly is in the workspace
str(bikesJuly)
# The outcome column
outcome
# The inputs to use
vars
# Create the formula string for bikes rented as a function of the inputs
(fmla <- paste(outcome, "~", paste(vars, collapse = " + "))) #Esta formula esta hecha con las varibles de arriba
# Calculate the mean and variance of the outcome
(mean_bikes <- mean(bikesJuly$cnt))
(var_bikes <- var(bikesJuly$cnt))
#Si la media y la varianza son mas o menos similares se usa como familia poisson, si no son parecidas se usa quasipoisson
#En este caso no son iguales
# Fit the model
# Notese que se usa como familia quasipoisson
bike_model <- glm(fmla, data = bikesJuly, family = quasipoisson)
# Call glance
#Usamos glance para obtener la desviacion y la desviacion nula para poder hacer el pseudo R2 y ver si nuestro modelo es bueno
(perf <- glance(bike_model))
# Calculate pseudo-R-squared
#El modelo es mas o menos bueno con 0.8 de pseudo R2.
#Pseudo R2 tiene que estar cerca de 1
(pseudoR2 <- 1 - (perf$deviance/perf$null.deviance))
#Predict bike rentals on new data
#Se realizan predicciones con la regresion logistica
View(bikesJuly)
#Fit a model of sparrow survival probability
#Se prueba la regresion logistica
#Esta regresion esta entre 0 y 1, utiliza el pseudo R2 para ver si se tiene un buen modelo
sparrow = readRDS("/home/marlon/Documents/machine_learning_R/Supervised Learning in R: Regression/sparrow.rds")
library(broom)
# sparrow is in the workspace
summary(sparrow)
# Create the survived column
sparrow$survived <- sparrow$status == "Survived" #Se reemplaza por true o false
# Create the formula
(fmla <- survived ~ total_length + weight + humerus)
# Fit the logistic regression model
sparrow_model <- glm(fmla, data = sparrow, family = binomial)
# Call summary
summary(sparrow_model)
# Call glance
(perf <- glance(sparrow_model)) #Permite sacar facilamente los pseudo R2
# Calculate pseudo-R-squared
(pseudoR2 <- 1 - (perf$deviance/ perf$null.deviance)) # No es un buen modelo porque no se acerca a 1
#Se hacen predicciones con el modelo de regresion logistica
#Se prueba la curva de ganancia
#Entre mas cerca este de la curva de ganancia es mejor
#GainCurvePlot(frame, xvar, truthVar, title)
# frame: data frame with prediction column and ground truth column
# xvar: the name of the column of predictions (as a string)
# truthVar: the name of the column with actual outcome (as a string)
# title: a title for the plot (as a string)
# sparrow is in the workspace
summary(sparrow)
# sparrow_model is in the workspace
summary(sparrow_model)
# Make predictions
sparrow$pred <- predict(sparrow_model, type = "response")
library(WVPlots)
# Look at gain curve
GainCurvePlot(sparrow, "pred", "survived", "sparrow survival model")
# You see from the gain curve that the model follows the wizard curve for about the first 30% of the data, identifying about 45% of the surviving sparrows with only a few false positives.
#Poisson and Quasipoisson
#Se usa para predecir datos de conteo
#Fit a model to predict bike rental counts
library(miceadds)
load.Rdata("/home/marlon/Documents/machine_learning_R/Supervised Learning in R: Regression/Bikes.RData", "bikesJuly")
#Para que hacemos estas variables???????
outcome = "cnt"
vars = c("hr","holiday", "workingday", "weathersit", "temp", "atemp", "hum", "windspeed")
#Las usamos para simplificar la formula que se va a utilizar
bikesAugust = bikesJuly
# bikesJuly is in the workspace
str(bikesJuly)
# The outcome column
outcome
# The inputs to use
vars
# Create the formula string for bikes rented as a function of the inputs
(fmla <- paste(outcome, "~", paste(vars, collapse = " + "))) #Esta formula esta hecha con las varibles de arriba
# Calculate the mean and variance of the outcome
(mean_bikes <- mean(bikesJuly$cnt))
(var_bikes <- var(bikesJuly$cnt))
#Si la media y la varianza son mas o menos similares se usa como familia poisson, si no son parecidas se usa quasipoisson
#En este caso no son iguales
# Fit the model
# Notese que se usa como familia quasipoisson
bike_model <- glm(fmla, data = bikesJuly, family = quasipoisson)
# Call glance
#Usamos glance para obtener la desviacion y la desviacion nula para poder hacer el pseudo R2 y ver si nuestro modelo es bueno
(perf <- glance(bike_model))
# Calculate pseudo-R-squared
#El modelo es mas o menos bueno con 0.8 de pseudo R2.
#Pseudo R2 tiene que estar cerca de 1
(pseudoR2 <- 1 - (perf$deviance/perf$null.deviance))
#Predict bike rentals on new data
#Se realizan predicciones con la regresion logistica
# bikesAugust is in the workspace
str(bikesAugust) #BikesAugust es la newdata
# bike_model is in the workspace
summary(bike_model)
# Make predictions on August data
bikesAugust$pred  <- predict(bike_model, newdata = bikesAugust, type = "response")
# Calculate the RMSE
bikesAugust %>%
mutate(residual = cnt - pred) %>%
summarize(rmse  = sqrt(mean(residual^2)))
# Plot predictions vs cnt (pred on x-axis)
ggplot(bikesAugust, aes(x = pred, y = cnt)) +
geom_point() +
geom_abline(color = "darkblue")
library(ggplot2)
ggplot(bikesAugust, aes(x = pred, y = cnt)) +
geom_point() +
geom_abline(color = "darkblue")
library(tidyr)
library(tidyr)
# Plot predictions and cnt by date/time
bikesAugust %>%
# set start to 0, convert unit to days
mutate(instant = (instant - min(instant))/24) %>%  #Se convierte a dias
# gather cnt and pred into a value column
gather(key = valuetype, value = value, cnt, pred) %>%
filter(instant < 14) %>% # restric to first 14 days
# plot value by instant
ggplot(aes(x = instant, y = value, color = valuetype, linetype = valuetype)) +
geom_point() +
geom_line() +
scale_x_continuous("Day", breaks = 0:14, labels = 0:14) +
scale_color_brewer(palette = "Dark2") +
ggtitle("Predicted August bike rentals, Quasipoisson model")
library(tidyr)
library(dplyr)
# Plot predictions and cnt by date/time
bikesAugust %>%
# set start to 0, convert unit to days
mutate(instant = (instant - min(instant))/24) %>%  #Se convierte a dias
# gather cnt and pred into a value column
gather(key = valuetype, value = value, cnt, pred) %>%
filter(instant < 14) %>% # restric to first 14 days
# plot value by instant
ggplot(aes(x = instant, y = value, color = valuetype, linetype = valuetype)) +
geom_point() +
geom_line() +
scale_x_continuous("Day", breaks = 0:14, labels = 0:14) +
scale_color_brewer(palette = "Dark2") +
ggtitle("Predicted August bike rentals, Quasipoisson model")
load.Rdata("/home/marlon/Documents/machine_learning_R/Supervised Learning in R: Regression/Soybean.RData", "soybean_train")
summary(soybean_train)
# soybean_train is in the workspace
summary(soybean_train)
# Plot weight vs Time (Time on x axis)
ggplot(soybean_train, aes(x = Time, y = weight)) +
geom_point()
# Load the package mgcv
library(mgcv)
# Create the formula
(fmla.gam <- weight ~ s(Time)) #Time no tiene una relacion lineal
# Fit the GAM Model
model.gam <- gam(fmla.gam, family = gaussian, data= soybean_train)
# Call summary() on model.lin and look for R-squared
summary(model.lin)
# Call summary() on model.gam and look for R-squared
summary(model.gam)
# Call plot() on model.gam
plot(model.gam)
# Call summary() on model.lin and look for R-squared
summary(model.lin)
#Fit a model of sparrow survival probability
#Se prueba la regresion logistica
#Esta regresion esta entre 0 y 1, utiliza el pseudo R2 para ver si se tiene un buen modelo
sparrow = readRDS("/home/marlon/Documents/machine_learning_R/Supervised Learning in R: Regression/sparrow.rds")
library(broom)
# sparrow is in the workspace
summary(sparrow)
# Create the survived column
sparrow$survived <- sparrow$status == "Survived" #Se reemplaza por true o false
# Create the formula
(fmla <- survived ~ total_length + weight + humerus)
# Fit the logistic regression model
sparrow_model <- glm(fmla, data = sparrow, family = binomial)
# Call summary
summary(sparrow_model)
# Call glance
(perf <- glance(sparrow_model)) #Permite sacar facilamente los pseudo R2
# Calculate pseudo-R-squared
(pseudoR2 <- 1 - (perf$deviance/ perf$null.deviance)) # No es un buen modelo porque no se acerca a 1
#Se hacen predicciones con el modelo de regresion logistica
#Se prueba la curva de ganancia
#Entre mas cerca este de la curva de ganancia es mejor
#GainCurvePlot(frame, xvar, truthVar, title)
# frame: data frame with prediction column and ground truth column
# xvar: the name of the column of predictions (as a string)
# truthVar: the name of the column with actual outcome (as a string)
# title: a title for the plot (as a string)
# sparrow is in the workspace
summary(sparrow)
# sparrow_model is in the workspace
summary(sparrow_model)
# Make predictions
sparrow$pred <- predict(sparrow_model, type = "response")
library(WVPlots)
# Look at gain curve
GainCurvePlot(sparrow, "pred", "survived", "sparrow survival model")
# You see from the gain curve that the model follows the wizard curve for about the first 30% of the data, identifying about 45% of the surviving sparrows with only a few false positives.
#Poisson and Quasipoisson
#Se usa para predecir datos de conteo
#Fit a model to predict bike rental counts
library(miceadds)
load.Rdata("/home/marlon/Documents/machine_learning_R/Supervised Learning in R: Regression/Bikes.RData", "bikesJuly")
#Para que hacemos estas variables???????
outcome = "cnt"
vars = c("hr","holiday", "workingday", "weathersit", "temp", "atemp", "hum", "windspeed")
#Las usamos para simplificar la formula que se va a utilizar
bikesAugust = bikesJuly
# bikesJuly is in the workspace
str(bikesJuly)
# The outcome column
outcome
# The inputs to use
vars
# Create the formula string for bikes rented as a function of the inputs
(fmla <- paste(outcome, "~", paste(vars, collapse = " + "))) #Esta formula esta hecha con las varibles de arriba
# Calculate the mean and variance of the outcome
(mean_bikes <- mean(bikesJuly$cnt))
(var_bikes <- var(bikesJuly$cnt))
#Si la media y la varianza son mas o menos similares se usa como familia poisson, si no son parecidas se usa quasipoisson
#En este caso no son iguales
# Fit the model
# Notese que se usa como familia quasipoisson
bike_model <- glm(fmla, data = bikesJuly, family = quasipoisson)
# Call glance
#Usamos glance para obtener la desviacion y la desviacion nula para poder hacer el pseudo R2 y ver si nuestro modelo es bueno
(perf <- glance(bike_model))
# Calculate pseudo-R-squared
#El modelo es mas o menos bueno con 0.8 de pseudo R2.
#Pseudo R2 tiene que estar cerca de 1
(pseudoR2 <- 1 - (perf$deviance/perf$null.deviance))
#Predict bike rentals on new data
#Se realizan predicciones con la regresion logistica
# bikesAugust is in the workspace
str(bikesAugust) #BikesAugust es la newdata
# bike_model is in the workspace
summary(bike_model)
# Make predictions on August data
bikesAugust$pred  <- predict(bike_model, newdata = bikesAugust, type = "response")
# Calculate the RMSE
bikesAugust %>%
mutate(residual = cnt - pred) %>%
summarize(rmse  = sqrt(mean(residual^2)))
library(ggplot2)
# Plot predictions vs cnt (pred on x-axis)
ggplot(bikesAugust, aes(x = pred, y = cnt)) +
geom_point() +
geom_abline(color = "darkblue")
#Visualize the bike rental predictions
#Se ve como el modelo performa en el tiempo
library(tidyr)
library(dplyr)
# Plot predictions and cnt by date/time
bikesAugust %>%
# set start to 0, convert unit to days
mutate(instant = (instant - min(instant))/24) %>%  #Se convierte a dias
# gather cnt and pred into a value column
gather(key = valuetype, value = value, cnt, pred) %>%
filter(instant < 14) %>% # restric to first 14 days
# plot value by instant
ggplot(aes(x = instant, y = value, color = valuetype, linetype = valuetype)) +
geom_point() +
geom_line() +
scale_x_continuous("Day", breaks = 0:14, labels = 0:14) +
scale_color_brewer(palette = "Dark2") +
ggtitle("Predicted August bike rentals, Quasipoisson model")
#GAMs
#Model soybean growth with GAM
library(miceadds)
load.Rdata("/home/marlon/Documents/machine_learning_R/Supervised Learning in R: Regression/Soybean.RData", "soybean")
N = nrow(soybean)
target = round(N * 0.75)
gp = runif(N)
library(miceadds)
load.Rdata("/home/marlon/Documents/machine_learning_R/Supervised Learning in R: Regression/Soybean.RData", "soybean")
N = nrow(soybean)
target = round(N * 0.75)
gp = runif(N)
soybean_train = soybean[gp < 0.75, ]
#test data
soybean_test = soybean[gp >= 0.75, ]
# soybean_train is in the workspace
summary(soybean_train)
summary(soybean_train)
# Plot weight vs Time (Time on x axis)
ggplot(soybean_train, aes(x = Time, y = weight)) +
geom_point()
# Load the package mgcv
library(mgcv)
# Create the formula
(fmla.gam <- weight ~ s(Time)) #Time no tiene una relacion lineal
# Fit the GAM Model
model.gam <- gam(fmla.gam, family = gaussian, data= soybean_train)
# Call summary() on model.lin and look for R-squared
#summary(model.lin) Este el modelo lineal
# Call summary() on model.gam and look for R-squared
summary(model.gam)
# Call plot() on model.gam
plot(model.gam)
# soybean_test is in the workspace
summary(soybean_test)
#Predict with the soybean model on test data
# soybean_test is in the workspace
summary(soybean_test)
# soybean_test is in the workspace
summary(soybean_test)
# Get predictions from linear model
soybean_test$pred.lin <- predict(model.lin, newdata = soybean_test)
# Get predictions from gam model
soybean_test$pred.gam <- as.numeric(predict(model.gam, newdata = soybean_test))
# Gather the predictions into a "long" dataset
soybean_long <- soybean_test %>%
gather(key = modeltype, value = pred, pred.lin, pred.gam)
# Calculate the rmse
soybean_long %>%
mutate(residual = weight - pred) %>%     # residuals
group_by(modeltype) %>%                  # group by modeltype
summarize(rmse = sqrt(mean(residual^2))) # calculate the RMSE
# Compare the predictions against actual weights on the test data
soybean_long %>%
ggplot(aes(x = Time)) +                          # the column for the x axis
geom_point(aes(y = weight)) +                    # the y-column for the scatterplot
geom_point(aes(y = pred, color = modeltype)) +   # the y-column for the point-and-line plot
geom_line(aes(y = pred, color = modeltype, linetype = modeltype)) + # the y-column for the point-and-line plot
scale_color_brewer(palette = "Dark2")
# Get predictions from linear model
soybean_test$pred.lin <- predict(model.lin, newdata = soybean_test)
(fmla.lin <- weight ~ Time)
model.lin <- gam(fmla.lin, family = gaussian, data= soybean_train)
# Load the package mgcv
library(mgcv)
# Create the formula
(fmla.gam <- weight ~ s(Time)) #Time no tiene una relacion lineal
# Fit the GAM Model
model.gam <- gam(fmla.gam, family = gaussian, data= soybean_train)
(fmla.lin <- weight ~ Time)
model.lin <- gam(fmla.lin, family = gaussian, data= soybean_train)
# Call summary() on model.lin and look for R-squared
#summary(model.lin) Este el modelo lineal
# Call summary() on model.gam and look for R-squared
summary(model.gam)
# Call plot() on model.gam
plot(model.gam)
#Predict with the soybean model on test data
# soybean_test is in the workspace
summary(soybean_test)
# soybean_test is in the workspace
summary(soybean_test)
# Get predictions from linear model
soybean_test$pred.lin <- predict(model.lin, newdata = soybean_test)
# Get predictions from gam model
soybean_test$pred.gam <- as.numeric(predict(model.gam, newdata = soybean_test))
# Gather the predictions into a "long" dataset
soybean_long <- soybean_test %>%
gather(key = modeltype, value = pred, pred.lin, pred.gam)
# Calculate the rmse
soybean_long %>%
mutate(residual = weight - pred) %>%     # residuals
group_by(modeltype) %>%                  # group by modeltype
summarize(rmse = sqrt(mean(residual^2))) # calculate the RMSE
# Compare the predictions against actual weights on the test data
soybean_long %>%
ggplot(aes(x = Time)) +                          # the column for the x axis
geom_point(aes(y = weight)) +                    # the y-column for the scatterplot
geom_point(aes(y = pred, color = modeltype)) +   # the y-column for the point-and-line plot
geom_line(aes(y = pred, color = modeltype, linetype = modeltype)) + # the y-column for the point-and-line plot
scale_color_brewer(palette = "Dark2")
head(livetweets_data)
library(readr)
livetweets_data <- read_delim("/home/marlon/mainfolder/marlon/USFQ/DataMining/3_TextMining/P3/livetweets_data.csv",
"|", escape_double = FALSE, trim_ws = TRUE)
head(livetweets_data)
library(dplyr)
library(dplyr)
livetweets_data = livetweets_data %>%
select(tweet_text, tweet_screen_name)
head(livetweets_data)
saveRDS(livetweets_data, file = "livetweets_data.rds")
saveRDS(livetweets_data, file = "/home/marlon/mainfolder/marlon/USFQ/DataMining/3_TextMining/P3/livetweets_data.rds")
View(df)
remove(list = ls())
library(dplyr)
library(readr)
library(ggplot2)
library(tidytext)
library(geometry)
library(gridExtra)
library(tidyverse)
library(tm)
library(stringr)
library(pdftools)
setwd("/home/marlon/mainfolder/marlon/USFQ/DataMining/3_TextMining/P3")
livetweets_data = readRDS("livetweets_data.rds")
account_info_data = read_csv("account_info.csv")
account_info_data = account_info_data %>%
select(nombre, apellidos, twitter_screen_name)
livetweets_data = livetweets_data %>%
filter(tweet_screen_name %in% account_info_data$twitter_screen_name)
#Pregunta 1
ranking_candidatos_mas_utilizaron = livetweets_data %>%
group_by(tweet_screen_name) %>%
summarise(count = n()) %>%
arrange(desc(count)) %>%
top_n(n = 10, count)
stop_words = c("que", "por","los","para","con", "del", "ante","las", "este", "una", "sus", "esta", "estas","mas", "pero", "ser","nos","soy", "usted", "desde",
"hoy","todo","eso", "tiene", "tengo", "son", "como", "hay", "asi", "sin", "fue", "debe", "estan", "muy", "les", "todos", "mis","antes",
"despues", "han","dia", "hasta", "esa", "dale", "via","voy", "cada", "donde", "todas", "todos", "gracias", "hace", "sobre", "nuestra",
"nuestras", "ahora", "hacer", "manana", "junto", "juntos", "nuestro", "porque", "vez", "tengo", "tengan","estuvo")
#Se elimina todos los hashtags menciones puntuacion [Normalizacion]
for(i in 1:length(livetweets_data$tweet_text)){
livetweets_data$tweet_text[i] = stringi::stri_trans_general(livetweets_data$tweet_text[i], "Latin-ASCII") #Acentos
livetweets_data$tweet_text[i] = removeWords(tolower(livetweets_data$tweet_text[i]), stop_words) #Stop Words
livetweets_data$tweet_text[i] = gsub('((f|ht)tp\\S+\\s*)|([^\x01-\x7F])|(@[A-Za-z0-9]*)|(#[A-Za-z0-9]*)|(([0-9])([^\\s]+))|([0-9])', "", livetweets_data$tweet_text[i])
livetweets_data$tweet_text[i] = gsub(" *\\b[[:alpha:]]{1,2}\\b *", " ", livetweets_data$tweet_text[i])
livetweets_data$tweet_text[i] = str_replace_all(livetweets_data$tweet_text[i], "[[:punct:]]", "")
}
ranking_palabras_mas_utilizadas = livetweets_data %>%
unnest_tokens(input = tweet_text, output = "words", format = "text", drop = T, to_lower = T) %>%
group_by(words, tweet_screen_name) %>%
summarise(count = n())
#Pregunta 2
top_10_ranking_palabras = ranking_palabras_mas_utilizadas %>%
filter(tweet_screen_name %in% ranking_candidatos_mas_utilizaron$tweet_screen_name) %>%
group_by(tweet_screen_name) %>%
top_n(count, n = 10) %>%
slice_head(n = 10) %>%
arrange(tweet_screen_name,desc(count))
#Pregunta 2
tf = ranking_palabras_mas_utilizadas %>%
filter(tweet_screen_name %in% ranking_candidatos_mas_utilizaron$tweet_screen_name) %>%
group_by(tweet_screen_name) %>%
mutate(total = sum(count))
tf_idf = tf %>%
group_by(words) %>%
mutate(tf = count/total, df = n(), idf = log10(length(ranking_candidatos_mas_utilizaron$tweet_screen_name)/df), tf_idf = tf * idf) %>%
ungroup() %>%
group_by(tweet_screen_name) %>%
top_n(count, n = 10) %>%
slice_head(n = 10) %>%
arrange(tweet_screen_name,desc(tf_idf))
View(tf_idf)
